{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607378b7-d7dd-45cd-a3cd-421eec534e35",
   "metadata": {},
   "source": [
    "# import vosk\n",
    "import pyaudio\n",
    "import json\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import wave\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668512b2-4052-4c3c-a314-d9a88299d94a",
   "metadata": {},
   "source": [
    "### Getting all the input devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f74c9b-aa20-4214-967d-6017870b8ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Audio Input Devices:\n",
      "Device 0: Microsoft Sound Mapper - Input\n",
      "Device 1: Microphone (Razer Seiren Mini)\n",
      "Device 2: Microphone (HD Webcam eMeet C96\n",
      "Device 8: Primary Sound Capture Driver\n",
      "Device 9: Microphone (Razer Seiren Mini)\n",
      "Device 10: Microphone (HD Webcam eMeet C960)\n",
      "Device 20: Microphone (HD Webcam eMeet C960)\n",
      "Device 21: Microphone (Razer Seiren Mini)\n",
      "Device 23: Stereo Mix (Realtek HD Audio Stereo input)\n",
      "Device 25: Line In (Realtek HD Audio Line input)\n",
      "Device 27: Microphone (Realtek HD Audio Mic input)\n",
      "Device 30: Input (OCULUSVAD Wave Speaker Headphone)\n",
      "Device 31: Headset Microphone (OCULUSVAD Wave Microphone Headphone)\n",
      "Device 32: Microphone (Razer Seiren Mini)\n",
      "Device 35: SteelSeries Sonar - Stream (SteelSeries_Sonar_VAD Stream Wave)\n",
      "Device 37: SteelSeries Sonar - Microphone (SteelSeries_Sonar_VAD Chat Capture Wave)\n",
      "Device 41: Microphone (VDVAD Wave)\n",
      "Device 45: Microphone (HD Webcam eMeet C960)\n",
      "Device 47: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(WH-1000XM3))\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Get the number of audio input devices\n",
    "device_count = p.get_device_count()\n",
    "\n",
    "print(\"Available Audio Input Devices:\")\n",
    "for i in range(device_count):\n",
    "    info = p.get_device_info_by_index(i)\n",
    "    if info['maxInputChannels'] > 0:  # Check if it's an input device\n",
    "        print(f\"Device {i}: {info['name']}\")\n",
    "\n",
    "# Terminate PyAudio\n",
    "p.terminate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5da237-b07b-4480-b6ff-393cc5191dcd",
   "metadata": {},
   "source": [
    "### setting index to set defalt input device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423eda1a-f07a-49c1-a691-aa8846962c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87908e5-bab3-4191-88b0-62599ae048de",
   "metadata": {},
   "source": [
    "## Using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478734f9-5137-4d66-b27b-fb67a65d383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./vosk-model-en-us-0.42-gigaspeech\"\n",
    "\n",
    "# # Check if the model path exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Model path does not exist.\")\n",
    "else:\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        # model = vosk.Model(model_path)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create a model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1fc8eb-0c69-4c25-a4b5-082d15f655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "def record_audio(device_index=0, model_path=\"model\"):\n",
    "    \"\"\"Records audio from the microphone, recognizes speech until 'stop' is detected, \n",
    "       and removes the final 'stop' word from the result.\"\"\"\n",
    "    \n",
    "    # Initialize the recognizer model (make sure model_path points to your Vosk model)\n",
    "    model = Model(model_path)\n",
    "    rec = KaldiRecognizer(model, 16000)\n",
    "    \n",
    "    # Initialize the audio stream\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=16000,\n",
    "                    input=True,\n",
    "                    input_device_index=device_index,\n",
    "                    frames_per_buffer=8192)\n",
    "    \n",
    "    print(f\"Using microphone: {p.get_device_info_by_index(device_index)['name']}\")\n",
    "    recognized_text = \"\"\n",
    "\n",
    "    # Stream audio and recognize speech\n",
    "    print(\"Listening for speech. Say 'Stop' to stop.\")\n",
    "    while True:\n",
    "        data = stream.read(4096, exception_on_overflow=False)\n",
    "        if rec.AcceptWaveform(data):\n",
    "            result = json.loads(rec.Result())\n",
    "            text = result.get(\"text\", \"\")\n",
    "            recognized_text += text + \" \"\n",
    "            print(text)\n",
    "            \n",
    "            # Stop condition\n",
    "            if \"stop\" in text.lower():\n",
    "                print(\"Stop keyword detected. Stopping...\")\n",
    "                break\n",
    "\n",
    "    # Clean up audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    # Remove the final 'stop' word\n",
    "    words = recognized_text.strip().split()\n",
    "    if words and words[-1].lower() == \"stop\":\n",
    "        words.pop()\n",
    "        words.pop()\n",
    "    \n",
    "    cleaned_text = \" \".join(words)\n",
    "    print(\"Last word 'stop' removed successfully!\")\n",
    "    \n",
    "    return cleaned_text  # Return the cleaned recognized text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50c43ac-8be3-44c6-996b-5afa4d28f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = vosk.Model(lang=\"en-in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ab5fe4-fdad-4579-8a9c-30400656bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a recognizer\n",
    "# rec = vosk.KaldiRecognizer(model, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa7dadf-660d-4cfd-85e7-cca103abebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the microphone stream\n",
    "# p = pyaudio.PyAudio()\n",
    "# stream = p.open(format=pyaudio.paInt16,\n",
    "#                 channels=1,\n",
    "#                 rate=16000,\n",
    "#                 input=True,\n",
    "#                 input_device_index= device_index,\n",
    "#                 frames_per_buffer=8192)\n",
    "# print(f\"Using microphone: {p.get_device_info_by_index(device_index)['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37fc326-33f7-4b8d-afa3-7851afde36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path for the output text file\n",
    "# output_file_path = \"recognized_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f177a5-48fc-408a-a15f-37cdfa20a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open a text file in write mode using a 'with' block\n",
    "# with open(output_file_path, \"w\") as output_file:\n",
    "#     print(\"Listening for speech. Say 'Stop' to stop.\")\n",
    "#     # Start streaming and recognize speech\n",
    "#     while True:\n",
    "#         data = stream.read(4096,exception_on_overflow= False)#read in chunks of 4096 bytes\n",
    "#         if rec.AcceptWaveform(data):#accept waveform of input voice\n",
    "#             # Parse the JSON result and get the recognized text\n",
    "#             result = json.loads(rec.Result())\n",
    "#             recognized_text = result['text']\n",
    "            \n",
    "#             # Write recognized text to the file\n",
    "#             output_file.write(recognized_text + \"\\n\")\n",
    "#             print(recognized_text)\n",
    "            \n",
    "#             # Check for the termination keyword\n",
    "#             if \"stop\" in recognized_text.lower():\n",
    "#                 print(\"Stop keyword detected. Stopping...\")\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4d1842-64b8-44f7-a3f2-0d830998c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop and close the stream\n",
    "# stream.stop_stream()\n",
    "# stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ff2f7d8-8e90-4319-8f6a-371546d326cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Terminate the PyAudio object\n",
    "# p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5a381-710e-4c28-904b-4cefeff1e0d5",
   "metadata": {},
   "source": [
    "### checking my own recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7824004d-d08c-4018-a317-ce3e8e16d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Set parameters for audio stream\n",
    "# FORMAT = pyaudio.paInt16  # Audio format (16-bit PCM)\n",
    "# CHANNELS = 1              # Number of audio channels\n",
    "# RATE = 16000              # Sample rate (16 kHz)\n",
    "# CHUNK = 1024              # Buffer size (number of frames per buffer)\n",
    "\n",
    "# # Create a PyAudio object\n",
    "# p = pyaudio.PyAudio()\n",
    "\n",
    "# # Open a stream to capture audio\n",
    "# stream = p.open(format=FORMAT,\n",
    "#                  channels=CHANNELS,\n",
    "#                  rate=RATE,\n",
    "#                  input=True,\n",
    "#                  frames_per_buffer=CHUNK)\n",
    "\n",
    "# print(\"Recording...\")\n",
    "\n",
    "# frames = []\n",
    "\n",
    "# # Record audio for a certain duration (e.g., 5 seconds)\n",
    "# for _ in range(0, int(RATE / CHUNK * 5)):  # Change 5 to the desired duration\n",
    "#     data = stream.read(CHUNK)\n",
    "#     frames.append(data)\n",
    "\n",
    "# print(\"Finished recording.\")\n",
    "\n",
    "# # Stop and close the stream\n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "# p.terminate()\n",
    "\n",
    "# # Save the recorded audio to a WAV file\n",
    "# output_file = \"output.wav\"\n",
    "# with wave.open(output_file, 'wb') as wf:\n",
    "#     wf.setnchannels(CHANNELS)\n",
    "#     wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "#     wf.setframerate(RATE)\n",
    "#     wf.writeframes(b''.join(frames))\n",
    "\n",
    "# print(f\"Audio saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f0418-a559-400c-b05a-74689f70610d",
   "metadata": {},
   "source": [
    "### Converting into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78316f98-1618-4d1c-b102-8a3299297a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"recognized_text.txt\", 'r') as file:\n",
    "#     text = \"\".join(line.rstrip() for line in file)\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834b8560-8dcf-4a2f-994c-8eeebf35cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def speech_to_text(audio_data, model_path=\"model\"):\n",
    "    \"\"\"Converts recorded audio data to text using Vosk speech recognition model.\n",
    "    \n",
    "    Args:\n",
    "        audio_data (bytes): Audio data to process.\n",
    "        model_path (str): Path to the Vosk model directory.\n",
    "\n",
    "    Returns:\n",
    "        str: Recognized text from the audio data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the recognizer model\n",
    "    model = Model(model_path)\n",
    "    rec = KaldiRecognizer(model, 16000)\n",
    "    \n",
    "    # Recognize speech from audio data\n",
    "    recognized_text = \"\"\n",
    "    \n",
    "    # Feed the audio data in chunks to the recognizer\n",
    "    if rec.AcceptWaveform(audio_data):\n",
    "        result = json.loads(rec.Result())\n",
    "        recognized_text = result.get(\"text\", \"\")\n",
    "    \n",
    "    print(\"Recognized Text:\", recognized_text)\n",
    "    return recognized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b87e72-e175-4d1d-85b5-226b6da5b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word deleted successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"recognized_text.txt\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "words = text.split()  # Split the text into a list of words\n",
    "if words:  # Ensure there's at least one word\n",
    "    words.pop()\n",
    "    words.pop()# Remove the last word\n",
    "\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\" \".join(words))  \n",
    "\n",
    "print(\"Last word deleted successfully!\") # deleting the last word as it is stop\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f3e98cc-e584-4f65-80b0-c57bf858a4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41508dc-155c-4e9d-bd6a-c4222b1aa0f0",
   "metadata": {},
   "source": [
    "### English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad7a9cbb-33f2-451d-8a62-cad6bae6f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amrit\\Documents\\Projects\\priyanka\\venv\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# english to french\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "src_text = text \n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors = 'pt', padding = True))\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens = True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1bffd4a-6176-4eac-8255-bd0d97851065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Le présent règlement entre en vigueur le jour suivant celui de sa publication au Journal officiel de l'Union européenne.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e341a6-f3a3-4d50-8d71-19bd8e0b2a2f",
   "metadata": {},
   "source": [
    "### French to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e96bc04-6da7-4554-8c20-b33dc88b7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# french to english\n",
    "\n",
    "\n",
    "src_text = tgt_text\n",
    "model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors = 'pt', padding = True))\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens = True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1af25c7d-4277-428d-b3eb-a20e5f87f5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Regulation shall enter into force on the day following its publication in the Official Journal of the European Union.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6fe2f-d782-4ccc-aa97-435434281b58",
   "metadata": {},
   "source": [
    "### English to Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2152df33-2c1f-46c7-88fd-5ee6299ad4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English to Spanish\n",
    "src_text = text\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors='pt', padding=True))\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7707347-d6a9-4241-8fa3-d8fc2f20793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd910e2f-7591-452d-8d46-5476736f8890",
   "metadata": {},
   "source": [
    "### Spanish to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3abba1f-37ad-4550-95bd-81e4285357f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish to English\n",
    "src_text = tgt_text\n",
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors='pt', padding=True))\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02621acc-ff31-4f9b-a0d2-25f7be2a8f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a95a0766-8b68-469a-b4a3-6039c5ba352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate_text(src_text, target_language_model):\n",
    "    \"\"\"Translates the source text to the desired language using the specified model.\n",
    "\n",
    "    Args:\n",
    "        src_text (str): The source text in the original language.\n",
    "        target_language_model (str): The name of the MarianMT model for the target language translation.\n",
    "\n",
    "    Returns:\n",
    "        str: Translated text in the target language.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the model and tokenizer for the target language\n",
    "    tokenizer = MarianTokenizer.from_pretrained(target_language_model)\n",
    "    model = MarianMTModel.from_pretrained(target_language_model)\n",
    "    \n",
    "    # Tokenize the source text and generate the translation\n",
    "    translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "    \n",
    "    # Decode and return the translated text\n",
    "    tgt_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    \n",
    "    return tgt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673783d-79d4-4120-8944-ed2a4ffddd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c10b9-4a03-45cf-ac28-f7fb66503622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4079b-436f-4f42-8fad-5443f3cad02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "190690d7-7c56-47fa-a0ee-9cf773784823",
   "metadata": {},
   "source": [
    "### Text to Speech using gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a053d71f-1d3e-4c17-9f3d-ab9303f8fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_text = \"\".join(tgt_text)\n",
    "# language = 'es'\n",
    "# # Passing the text and language to the engine, \n",
    "# # here we have marked slow=False. Which tells \n",
    "# # the module that the converted audio should \n",
    "# # have a high speed\n",
    "# myobj = gTTS(text = my_text, lang = language, slow = False)\n",
    "# myobj.save(\"target.mp3\")\n",
    "# os.system(\"target.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70ef69a7-ebda-414d-ad8d-4b61478097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "\n",
    "def text_to_speech(text, language_code):\n",
    "    \"\"\"Converts text to speech in the specified language and returns the audio data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to convert to speech.\n",
    "        language_code (str): The language code for the desired speech language (e.g., \"en\" for English, \"fr\" for French).\n",
    "        \n",
    "    Returns:\n",
    "        BytesIO: In-memory audio file with the spoken text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to speech\n",
    "    tts = gTTS(text=text, lang=language_code)\n",
    "    \n",
    "    # Save audio to an in-memory file\n",
    "    audio_fp = BytesIO()\n",
    "    tts.write_to_fp(audio_fp)\n",
    "    audio_fp.seek(0)  # Reset file pointer to the beginning\n",
    "    \n",
    "    return audio_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4032c-bbdb-482a-8706-e2fba23beff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63c70b-57c9-4b46-821c-9a041032ae37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56795cd6-c28f-43ad-8f31-4ac74e3d1ee1",
   "metadata": {},
   "source": [
    "### Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa380a92-566b-468e-b730-0befa2aaf75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit.py\n",
    "# import streamlit as st\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Assuming the following functions exist and are directly callable:\n",
    "# # - record_audio() -> records English audio and returns audio data\n",
    "# # - speech_to_text(audio_data) -> converts recorded audio to text\n",
    "# # - translate_text(src_text, target_language_model) -> translates text to the desired language\n",
    "# # - text_to_speech(text, language_code) -> converts text to speech and returns audio file\n",
    "\n",
    "# # Language options dictionary, mapping language names to translation model names and language codes\n",
    "# language_options = {\n",
    "#     \"French\": (\"Helsinki-NLP/opus-mt-en-fr\", \"fr\"),\n",
    "#     \"Spanish\": (\"Helsinki-NLP/opus-mt-en-es\", \"es\")\n",
    "# }\n",
    "\n",
    "# # Streamlit GUI setup\n",
    "# st.title(\"Multilingual Audio Translator\")\n",
    "\n",
    "# # Step 1: Record Audio\n",
    "# st.header(\"Step 1: Record English Audio\")\n",
    "# if st.button(\"Start Recording\"):\n",
    "#     st.info(\"Recording... Please speak now.\")\n",
    "#     audio_data = record_audio()  # Use your function to record audio\n",
    "#     st.success(\"Recording complete.\")\n",
    "    \n",
    "#     # Convert recorded audio to text (Speech-to-Text)\n",
    "#     try:\n",
    "#         src_text = speech_to_text(audio_data)  # Convert to text using your function\n",
    "#         st.write(\"Recognized Text:\", src_text)\n",
    "#     except Exception as e:\n",
    "#         st.error(f\"Error: {e}\")\n",
    "\n",
    "#     # Step 2: Choose Translation Language\n",
    "#     st.header(\"Step 2: Translate Text\")\n",
    "#     target_language = st.selectbox(\"Select target language:\", list(language_options.keys()))\n",
    "    \n",
    "#     if target_language and src_text:\n",
    "#         model_name, lang_code = language_options[target_language]\n",
    "#         translation = translate_text(src_text, model_name)  # Translate using your function\n",
    "#         st.write(\"Translated Text:\", translation)\n",
    "\n",
    "#         # Step 3: Text-to-Speech\n",
    "#         st.header(\"Step 3: Play Translated Speech\")\n",
    "#         audio_fp = text_to_speech(translation, lang_code)  # Convert to speech using your function\n",
    "#         st.audio(audio_fp, format=\"audio/mp3\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2043724-f97e-4e8c-bf21-23f91bc90341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # Translation function\n",
    "# def translate_text(src_text, model_name):\n",
    "#     tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "#     model = MarianMTModel.from_pretrained(model_name)\n",
    "#     translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "#     tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "#     return tgt_text[0]\n",
    "\n",
    "# # Streamlit GUI\n",
    "# st.title(\"Multilingual Translator\")\n",
    "\n",
    "# # Language options\n",
    "# language_options = {\n",
    "#     \"English to French\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "#     \"French to English\": \"Helsinki-NLP/opus-mt-fr-en\",\n",
    "#     \"Spanish to English\": \"Helsinki-NLP/opus-mt-es-en\",\n",
    "#     \"English to Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "# }\n",
    "\n",
    "# # User inputs\n",
    "# st.subheader(\"Select Translation Direction\")\n",
    "# translation_direction = st.selectbox(\"Choose a translation direction:\", list(language_options.keys()))\n",
    "\n",
    "# st.subheader(\"Enter Text to Translate\")\n",
    "# src_text = st.text_area(\"Input text here\")\n",
    "\n",
    "# # Translate button\n",
    "# if st.button(\"Translate\"):\n",
    "#     if src_text:\n",
    "#         model_name = language_options[translation_direction]\n",
    "#         translation = translate_text(src_text, model_name)\n",
    "#         st.subheader(\"Translated Text\")\n",
    "#         st.write(translation)\n",
    "#     else:\n",
    "#         st.warning(\"Please enter text to translate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d24f4-bb80-48e3-82c6-9a785da4dd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
